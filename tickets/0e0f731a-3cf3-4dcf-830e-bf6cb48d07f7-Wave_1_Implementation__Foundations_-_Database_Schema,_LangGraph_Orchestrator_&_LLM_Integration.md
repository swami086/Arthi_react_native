---
id: "0e0f731a-3cf3-4dcf-830e-bf6cb48d07f7"
title: "Wave 1 Implementation: Foundations - Database Schema, LangGraph Orchestrator & LLM Integration"
assignee: ""
status: 0
createdAt: "1768116345817"
updatedAt: "1768116905528"
type: ticket
---

# Wave 1 Implementation: Foundations - Database Schema, LangGraph Orchestrator & LLM Integration

# Wave 1 Implementation: Foundations

## Overview

Wave 1 establishes the foundational infrastructure for the AI agent system by extending the existing Supabase schema, setting up LangGraph.js orchestration, integrating LLM APIs, and implementing feature flags. This wave builds on your existing infrastructure (25 migrations, Rollbar monitoring, Edge Functions).

**Duration:** 1 week  
**Team Size:** 4 developers (Backend, Agent Infra, DevOps, Integration)  
**Effort:** ~40 hours

## Prerequisites

### Existing Infrastructure (Already Implemented ✅)

- Supabase project with 25 migrations
- Edge Functions infrastructure (Deno runtime)
- Rollbar monitoring (mobile/supabase/functions/_shared/rollbar.ts)
- Authentication system
- Database with RLS policies

### Required Environment Variables

```bash
# Add to .env files
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
POSTHOG_API_KEY=phc-...
NEXT_PUBLIC_POSTHOG_HOST=https://app.posthog.com
```

## Implementation Steps

### Step 1: Extend Database Schema for Agent System

**Related Ticket:** ticket:d969320e-d519-47a7-a258-e04789b8ce0e/7d72e40c-fd7f-4083-99b0-ff10e93c4a25

**Files to Create:**

#### 1.1 Enable pgvector Extension

Create mobile/supabase/migrations/026_enable_pgvector.sql:

```sql
-- Enable pgvector for RAG system
CREATE EXTENSION IF NOT EXISTS vector;

-- Verify installation
SELECT * FROM pg_extension WHERE extname = 'vector';
```

#### 1.2 Create Agent Conversations Table

Create mobile/supabase/migrations/027_agent_conversations.sql:

```sql
-- Agent conversations table
CREATE TABLE agent_conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  agent_type TEXT NOT NULL CHECK (agent_type IN ('booking', 'session', 'insights', 'followup', 'general')),
  messages JSONB NOT NULL DEFAULT '[]',
  context JSONB DEFAULT '{}',
  status TEXT DEFAULT 'active' CHECK (status IN ('active', 'completed', 'escalated', 'failed')),
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_agent_conversations_user ON agent_conversations(user_id);
CREATE INDEX idx_agent_conversations_status ON agent_conversations(status);
CREATE INDEX idx_agent_conversations_created ON agent_conversations(created_at DESC);
CREATE INDEX idx_agent_conversations_agent_type ON agent_conversations(agent_type);

-- RLS Policies
ALTER TABLE agent_conversations ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own conversations"
  ON agent_conversations FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can create own conversations"
  ON agent_conversations FOR INSERT
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own conversations"
  ON agent_conversations FOR UPDATE
  USING (auth.uid() = user_id);

-- Trigger for updated_at
CREATE TRIGGER update_agent_conversations_updated_at
  BEFORE UPDATE ON agent_conversations
  FOR EACH ROW
  EXECUTE FUNCTION update_updated_at_column();
```

#### 1.3 Create Agent Memory Table (RAG)

Create mobile/supabase/migrations/028_agent_memory.sql:

```sql
-- Agent memory table for RAG
CREATE TABLE agent_memory (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID REFERENCES agent_conversations(id) ON DELETE CASCADE,
  content TEXT NOT NULL,
  embedding VECTOR(1536), -- OpenAI text-embedding-3-small dimensions
  metadata JSONB DEFAULT '{}',
  document_id UUID,
  chunk_index INT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Vector similarity search index (HNSW for better performance)
CREATE INDEX ON agent_memory USING hnsw (embedding vector_cosine_ops);

-- Other indexes
CREATE INDEX idx_agent_memory_conversation ON agent_memory(conversation_id);
CREATE INDEX idx_agent_memory_document ON agent_memory(document_id);
CREATE INDEX idx_agent_memory_created ON agent_memory(created_at DESC);

-- RLS Policies
ALTER TABLE agent_memory ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own memory"
  ON agent_memory FOR SELECT
  USING (
    EXISTS (
      SELECT 1 FROM agent_conversations
      WHERE id = agent_memory.conversation_id
      AND user_id = auth.uid()
    )
  );

-- Function for semantic search
CREATE OR REPLACE FUNCTION match_documents(
  query_embedding VECTOR(1536),
  match_count INT DEFAULT 5,
  filter JSONB DEFAULT '{}'
)
RETURNS TABLE (
  id UUID,
  content TEXT,
  metadata JSONB,
  similarity FLOAT
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    agent_memory.id,
    agent_memory.content,
    agent_memory.metadata,
    1 - (agent_memory.embedding <=> query_embedding) AS similarity
  FROM agent_memory
  WHERE agent_memory.metadata @> filter
  ORDER BY agent_memory.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

#### 1.4 Create Agent Tools Registry

Create mobile/supabase/migrations/029_agent_tools.sql:

```sql
-- Agent tools registry
CREATE TABLE agent_tools (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT UNIQUE NOT NULL,
  description TEXT NOT NULL,
  parameters JSONB NOT NULL,
  permissions TEXT[] DEFAULT '{}',
  rate_limit JSONB DEFAULT '{"calls": 100, "window": "1h"}',
  enabled BOOLEAN DEFAULT true,
  version TEXT DEFAULT '1.0.0',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_agent_tools_name ON agent_tools(name);
CREATE INDEX idx_agent_tools_enabled ON agent_tools(enabled);

-- Seed initial tools
INSERT INTO agent_tools (name, description, parameters, permissions) VALUES
('search_therapists', 'Search for therapists by specialty and location', 
 '{"type": "object", "properties": {"specialty": {"type": "string"}, "location": {"type": "string"}}}',
 ARRAY['patient', 'therapist']),
('get_availability', 'Get therapist availability for booking',
 '{"type": "object", "properties": {"therapist_id": {"type": "string"}, "date_range": {"type": "string"}}}',
 ARRAY['patient', 'therapist']),
('book_appointment', 'Book an appointment with a therapist',
 '{"type": "object", "properties": {"therapist_id": {"type": "string"}, "patient_id": {"type": "string"}, "slot": {"type": "string"}}}',
 ARRAY['patient']);
```

#### 1.5 Create Agent Executions Log

Create mobile/supabase/migrations/030_agent_executions.sql:

```sql
-- Agent executions log for monitoring and cost tracking
CREATE TABLE agent_executions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID REFERENCES agent_conversations(id) ON DELETE CASCADE,
  agent_type TEXT NOT NULL,
  tool_calls JSONB DEFAULT '[]',
  tokens_used INT DEFAULT 0,
  cost_usd DECIMAL(10, 4) DEFAULT 0,
  duration_ms INT,
  status TEXT DEFAULT 'success' CHECK (status IN ('success', 'failed', 'timeout')),
  error TEXT,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_agent_executions_conversation ON agent_executions(conversation_id);
CREATE INDEX idx_agent_executions_created ON agent_executions(created_at DESC);
CREATE INDEX idx_agent_executions_status ON agent_executions(status);
CREATE INDEX idx_agent_executions_agent_type ON agent_executions(agent_type);

-- RLS Policies
ALTER TABLE agent_executions ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own executions"
  ON agent_executions FOR SELECT
  USING (
    EXISTS (
      SELECT 1 FROM agent_conversations
      WHERE id = agent_executions.conversation_id
      AND user_id = auth.uid()
    )
  );
```

#### 1.6 Create Proactive Notifications Table

Create mobile/supabase/migrations/031_proactive_notifications.sql:

```sql
-- Proactive notifications from agents
CREATE TABLE proactive_notifications (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  agent_type TEXT NOT NULL,
  notification_type TEXT NOT NULL CHECK (notification_type IN ('session_prep', 'insight', 'alert', 'followup')),
  title TEXT NOT NULL,
  message TEXT NOT NULL,
  action JSONB,
  priority TEXT DEFAULT 'normal' CHECK (priority IN ('low', 'normal', 'high', 'urgent')),
  read BOOLEAN DEFAULT false,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_proactive_notifications_user ON proactive_notifications(user_id);
CREATE INDEX idx_proactive_notifications_read ON proactive_notifications(read);
CREATE INDEX idx_proactive_notifications_created ON proactive_notifications(created_at DESC);
CREATE INDEX idx_proactive_notifications_type ON proactive_notifications(notification_type);

-- RLS Policies
ALTER TABLE proactive_notifications ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own notifications"
  ON proactive_notifications FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can update own notifications"
  ON proactive_notifications FOR UPDATE
  USING (auth.uid() = user_id);
```

#### 1.7 Create User Agent Preferences

Create mobile/supabase/migrations/032_user_agent_preferences.sql:

```sql
-- User preferences for AI agents
CREATE TABLE user_agent_preferences (
  user_id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,
  enabled_agents TEXT[] DEFAULT ARRAY['booking', 'session', 'insights', 'followup'],
  notification_frequency TEXT DEFAULT 'normal' CHECK (notification_frequency IN ('minimal', 'reduced', 'normal')),
  quiet_hours JSONB DEFAULT '{"start": "22:00", "end": "08:00"}',
  data_sharing_consent BOOLEAN DEFAULT true,
  transparency_level TEXT DEFAULT 'detailed' CHECK (transparency_level IN ('simple', 'detailed', 'technical')),
  classic_mode BOOLEAN DEFAULT false,
  onboarding_completed BOOLEAN DEFAULT false,
  language_preference TEXT DEFAULT 'en' CHECK (language_preference IN ('en', 'hi', 'hinglish')),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- RLS Policies
ALTER TABLE user_agent_preferences ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can manage own preferences"
  ON user_agent_preferences FOR ALL
  USING (auth.uid() = user_id);

-- Trigger to create default preferences for new users
CREATE OR REPLACE FUNCTION create_default_agent_preferences()
RETURNS TRIGGER AS $$
BEGIN
  INSERT INTO user_agent_preferences (user_id)
  VALUES (NEW.id)
  ON CONFLICT (user_id) DO NOTHING;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

CREATE TRIGGER on_auth_user_created_agent_prefs
  AFTER INSERT ON auth.users
  FOR EACH ROW
  EXECUTE FUNCTION create_default_agent_preferences();
```

#### 1.8 Create Feature Flags Table

Create mobile/supabase/migrations/033_feature_flags.sql:

```sql
-- Feature flags for gradual rollout
CREATE TABLE feature_flags (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT UNIQUE NOT NULL,
  description TEXT,
  enabled BOOLEAN DEFAULT false,
  rollout_percentage INT DEFAULT 0 CHECK (rollout_percentage >= 0 AND rollout_percentage <= 100),
  target_users UUID[] DEFAULT '{}',
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_feature_flags_name ON feature_flags(name);
CREATE INDEX idx_feature_flags_enabled ON feature_flags(enabled);

-- Seed initial feature flags
INSERT INTO feature_flags (name, description, enabled, rollout_percentage) VALUES
('ai_alpha_users', 'Alpha rollout (10% users)', false, 10),
('ai_beta_users', 'Beta rollout (50% users)', false, 50),
('ai_ga_enabled', 'General availability (100% users)', false, 100),
('booking_agent_enabled', 'Enable BookingAgent', false, 0),
('session_agent_enabled', 'Enable SessionAgent', false, 0),
('insights_agent_enabled', 'Enable InsightsAgent', false, 0),
('followup_agent_enabled', 'Enable FollowupAgent', false, 0),
('ai_chat_enabled', 'Enable embedded chat UI', false, 0),
('proactive_notifications', 'Enable proactive notifications', false, 0),
('generative_ui', 'Enable generative UI components', false, 0);

-- Function to check if feature is enabled for user
CREATE OR REPLACE FUNCTION is_feature_enabled(
  flag_name TEXT,
  check_user_id UUID
)
RETURNS BOOLEAN AS $$
DECLARE
  flag_record RECORD;
  user_hash INT;
BEGIN
  SELECT * INTO flag_record
  FROM feature_flags
  WHERE name = flag_name;

  IF NOT FOUND OR NOT flag_record.enabled THEN
    RETURN FALSE;
  END IF;

  -- Check if user is in target list
  IF check_user_id = ANY(flag_record.target_users) THEN
    RETURN TRUE;
  END IF;

  -- Percentage-based rollout
  IF flag_record.rollout_percentage >= 100 THEN
    RETURN TRUE;
  END IF;

  IF flag_record.rollout_percentage <= 0 THEN
    RETURN FALSE;
  END IF;

  -- Hash user ID for consistent assignment
  user_hash := ('x' || substr(md5(check_user_id::text), 1, 8))::bit(32)::int;
  RETURN (abs(user_hash) % 100) < flag_record.rollout_percentage;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

**Testing:**

```bash
# Run migrations
cd mobile
supabase db push

# Verify tables created
supabase db diff

# Test feature flag function
psql -h localhost -p 54322 -U postgres -d postgres -c "
  SELECT is_feature_enabled('ai_alpha_users', 'some-user-uuid');
"
```

### Step 2: Setup LangGraph.js Orchestration System

**Related Ticket:** ticket:d969320e-d519-47a7-a258-e04789b8ce0e/31fff4b4-78dc-43c0-ad41-f04770463328

**Latest Tech Stack:**

- LangGraph.js (latest version for Deno)
- Anthropic Claude Sonnet 4.5 (replaces 3.5 Sonnet)
- OpenAI GPT-4 Turbo

#### 2.1 Create Shared LLM Client

Create mobile/supabase/functions/_shared/llm-client.ts:

```typescript
// @ts-nocheck
import Anthropic from 'npm:@anthropic-ai/sdk@0.32.0';
import OpenAI from 'npm:openai@4.77.0';
import { reportError, reportInfo } from './rollbar.ts';

// Initialize clients
const anthropic = new Anthropic({
  apiKey: Deno.env.get('ANTHROPIC_API_KEY'),
});

const openai = new OpenAI({
  apiKey: Deno.env.get('OPENAI_API_KEY'),
});

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
  tools?: any[];
}

export interface LLMResponse {
  content: string;
  toolCalls?: any[];
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  cost?: number;
}

// Cost per 1M tokens (as of Jan 2026)
const COSTS = {
  'gpt-4-turbo': { input: 10, output: 30 }, // $10/$30 per 1M tokens
  'claude-sonnet-4-5': { input: 3, output: 15 }, // $3/$15 per 1M tokens
  'gpt-3.5-turbo': { input: 0.5, output: 1.5 }, // $0.50/$1.50 per 1M tokens
};

function calculateCost(model: string, promptTokens: number, completionTokens: number): number {
  const modelKey = model.includes('gpt-4') ? 'gpt-4-turbo' :
                   model.includes('claude') ? 'claude-sonnet-4-5' :
                   'gpt-3.5-turbo';
  
  const rate = COSTS[modelKey];
  return (promptTokens * rate.input + completionTokens * rate.output) / 1_000_000;
}

/**
 * Call LLM with automatic fallback from OpenAI to Anthropic
 */
export async function callLLM(
  messages: LLMMessage[],
  options: LLMOptions = {},
  traceId?: string
): Promise<LLMResponse> {
  const {
    model = 'gpt-4-turbo',
    temperature = 0.7,
    maxTokens = 4096,
    stream = false,
    tools = [],
  } = options;

  try {
    // Try OpenAI first
    reportInfo('Calling OpenAI API', 'llm-client:openai', { model, traceId });
    return await callOpenAI(messages, { model, temperature, maxTokens, stream, tools });
  } catch (error) {
    reportError(error, 'llm-client:openai-failed', { model, traceId });
    console.error('OpenAI failed, falling back to Anthropic', error);
    
    // Fallback to Anthropic Claude Sonnet 4.5
    reportInfo('Falling back to Anthropic', 'llm-client:anthropic-fallback', { traceId });
    return await callAnthropic(messages, { 
      model: 'claude-sonnet-4-5-20250929', 
      temperature, 
      maxTokens, 
      stream, 
      tools 
    });
  }
}

async function callOpenAI(
  messages: LLMMessage[],
  options: LLMOptions
): Promise<LLMResponse> {
  const response = await openai.chat.completions.create({
    model: options.model || 'gpt-4-turbo',
    messages: messages.map(m => ({
      role: m.role,
      content: m.content,
    })),
    temperature: options.temperature,
    max_tokens: options.maxTokens,
    tools: options.tools?.length > 0 ? options.tools : undefined,
    stream: options.stream,
  });

  if (options.stream) {
    // Return stream object for streaming responses
    return { content: '', stream: response };
  }

  const choice = response.choices[0];
  const usage = response.usage;

  return {
    content: choice.message.content || '',
    toolCalls: choice.message.tool_calls,
    usage: {
      promptTokens: usage?.prompt_tokens || 0,
      completionTokens: usage?.completion_tokens || 0,
      totalTokens: usage?.total_tokens || 0,
    },
    cost: calculateCost(
      options.model || 'gpt-4-turbo',
      usage?.prompt_tokens || 0,
      usage?.completion_tokens || 0
    ),
  };
}

async function callAnthropic(
  messages: LLMMessage[],
  options: LLMOptions
): Promise<LLMResponse> {
  // Extract system message
  const systemMessage = messages.find(m => m.role === 'system');
  const conversationMessages = messages.filter(m => m.role !== 'system');

  const response = await anthropic.messages.create({
    model: options.model || 'claude-sonnet-4-5-20250929',
    max_tokens: options.maxTokens || 4096,
    temperature: options.temperature,
    system: systemMessage?.content,
    messages: conversationMessages.map(m => ({
      role: m.role === 'assistant' ? 'assistant' : 'user',
      content: m.content,
    })),
    tools: options.tools?.length > 0 ? options.tools : undefined,
    stream: options.stream,
  });

  if (options.stream) {
    return { content: '', stream: response };
  }

  const content = response.content[0];
  const usage = response.usage;

  return {
    content: content.type === 'text' ? content.text : '',
    toolCalls: response.content.filter(c => c.type === 'tool_use'),
    usage: {
      promptTokens: usage.input_tokens,
      completionTokens: usage.output_tokens,
      totalTokens: usage.input_tokens + usage.output_tokens,
    },
    cost: calculateCost(
      'claude-sonnet-4-5',
      usage.input_tokens,
      usage.output_tokens
    ),
  };
}

export { anthropic, openai };
```

#### 2.2 Create Agent Orchestrator

Create mobile/supabase/functions/agent-orchestrator/index.ts:

```typescript
// @ts-nocheck
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';
import { corsHeaders } from '../_shared/cors.ts';
import { reportError, reportInfo, extractTraceContext } from '../_shared/rollbar.ts';
import { callLLM } from '../_shared/llm-client.ts';
import { classifyIntent } from './intent-classifier.ts';
import { getAgentRegistry } from './agent-registry.ts';

console.log('Agent Orchestrator Function Up and Running!');

serve(async (req) => {
  const { traceId, spanId } = extractTraceContext(req);
  
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders });
  }

  try {
    const { message, context, conversationId } = await req.json();
    
    if (!message) {
      throw new Error('Message is required');
    }

    const supabase = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    );

    // Get or create conversation
    let conversation;
    if (conversationId) {
      const { data } = await supabase
        .from('agent_conversations')
        .select('*')
        .eq('id', conversationId)
        .single();
      conversation = data;
    }

    if (!conversation) {
      // Create new conversation
      const { data: newConv } = await supabase
        .from('agent_conversations')
        .insert({
          user_id: context.userId,
          agent_type: 'general',
          messages: [],
          context: context || {},
        })
        .select()
        .single();
      conversation = newConv;
    }

    // Classify intent
    reportInfo('Classifying user intent', 'orchestrator:classify', { traceId });
    const { intent, confidence } = await classifyIntent(message, context);

    reportInfo(`Intent classified: ${intent} (${confidence}%)`, 'orchestrator:intent', {
      intent,
      confidence,
      traceId,
    });

    // Get appropriate agent
    const agentRegistry = getAgentRegistry();
    const agent = agentRegistry[intent] || agentRegistry['general'];

    // Execute agent
    reportInfo(`Executing ${agent.name}`, 'orchestrator:execute', { traceId });
    const startTime = Date.now();
    
    const response = await agent.execute({
      message,
      context,
      conversation,
      supabase,
    });

    const duration = Date.now() - startTime;

    // Update conversation
    const updatedMessages = [
      ...conversation.messages,
      { role: 'user', content: message, timestamp: new Date().toISOString() },
      { role: 'assistant', content: response.content, timestamp: new Date().toISOString() },
    ];

    await supabase
      .from('agent_conversations')
      .update({
        messages: updatedMessages,
        agent_type: intent,
        updated_at: new Date().toISOString(),
      })
      .eq('id', conversation.id);

    // Log execution
    await supabase
      .from('agent_executions')
      .insert({
        conversation_id: conversation.id,
        agent_type: intent,
        tool_calls: response.toolCalls || [],
        tokens_used: response.usage?.totalTokens || 0,
        cost_usd: response.cost || 0,
        duration_ms: duration,
        status: 'success',
        metadata: {
          confidence,
          model: response.model || 'gpt-4-turbo',
        },
      });

    reportInfo('Agent execution completed', 'orchestrator:complete', {
      duration,
      tokens: response.usage?.totalTokens,
      cost: response.cost,
      traceId,
    });

    return new Response(
      JSON.stringify({
        conversationId: conversation.id,
        agentType: intent,
        response: response.content,
        toolCalls: response.toolCalls,
        confidence,
        reasoning: response.reasoning,
        usage: response.usage,
        cost: response.cost,
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );

  } catch (error) {
    console.error('Orchestrator Error:', error);
    await reportError(error, 'orchestrator', { traceId, spanId });

    return new Response(
      JSON.stringify({
        error: 'Internal server error',
        trace_id: traceId,
      }),
      {
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      }
    );
  }
});
```

#### 2.3 Create Intent Classifier

Create mobile/supabase/functions/agent-orchestrator/intent-classifier.ts:

```typescript
// @ts-nocheck
import { callLLM } from '../_shared/llm-client.ts';

export interface IntentResult {
  intent: 'booking' | 'session' | 'insights' | 'followup' | 'general';
  confidence: number;
  reasoning: string[];
}

export async function classifyIntent(
  message: string,
  context: any
): Promise<IntentResult> {
  const systemPrompt = `You are an intent classifier for a therapy platform. 
Classify the user's message into one of these intents:
- booking: User wants to book/schedule an appointment
- session: Related to ongoing therapy sessions, SOAP notes, or session management
- insights: Requesting analytics, progress reports, or dashboard insights
- followup: Post-session follow-up, mood tracking, or feedback
- general: General questions, help, or other topics

Respond with JSON: {"intent": "booking", "confidence": 85, "reasoning": ["User mentioned 'book'", "Mentioned anxiety specialty"]}`;

  const response = await callLLM(
    [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: `Context: ${JSON.stringify(context)}\n\nMessage: ${message}` },
    ],
    {
      model: 'gpt-3.5-turbo', // Use cheaper model for classification
      temperature: 0.1,
      maxTokens: 200,
    }
  );

  try {
    const result = JSON.parse(response.content);
    return {
      intent: result.intent,
      confidence: result.confidence,
      reasoning: result.reasoning || [],
    };
  } catch (error) {
    // Fallback to keyword-based classification
    const lowerMessage = message.toLowerCase();
    
    if (lowerMessage.includes('book') || lowerMessage.includes('appointment') || lowerMessage.includes('schedule')) {
      return { intent: 'booking', confidence: 70, reasoning: ['Keyword: book/appointment'] };
    }
    
    if (lowerMessage.includes('session') || lowerMessage.includes('soap') || lowerMessage.includes('notes')) {
      return { intent: 'session', confidence: 70, reasoning: ['Keyword: session/soap'] };
    }
    
    if (lowerMessage.includes('insight') || lowerMessage.includes('progress') || lowerMessage.includes('analytics')) {
      return { intent: 'insights', confidence: 70, reasoning: ['Keyword: insights/progress'] };
    }
    
    if (lowerMessage.includes('followup') || lowerMessage.includes('feeling') || lowerMessage.includes('mood')) {
      return { intent: 'followup', confidence: 70, reasoning: ['Keyword: followup/mood'] };
    }
    
    return { intent: 'general', confidence: 50, reasoning: ['No clear intent detected'] };
  }
}
```

#### 2.4 Create Agent Registry

Create mobile/supabase/functions/agent-orchestrator/agent-registry.ts:

```typescript
// @ts-nocheck
import { callLLM } from '../_shared/llm-client.ts';

export interface Agent {
  name: string;
  description: string;
  systemPrompt: string;
  execute: (params: any) => Promise<any>;
}

export function getAgentRegistry(): Record<string, Agent> {
  return {
    general: {
      name: 'GeneralAgent',
      description: 'Handles general queries and help requests',
      systemPrompt: `You are a helpful AI assistant for TherapyFlow, a therapy platform.
You can help users with:
- General questions about the platform
- Navigation and feature explanations
- Troubleshooting common issues

Be friendly, concise, and helpful. If the user needs specialized help (booking, sessions, etc.), 
suggest they use the appropriate quick action (@book, @insights, etc.).`,
      execute: async ({ message, context, conversation, supabase }) => {
        const messages = [
          { role: 'system', content: getAgentRegistry().general.systemPrompt },
          ...conversation.messages.slice(-5).map(m => ({ role: m.role, content: m.content })),
          { role: 'user', content: message },
        ];

        const response = await callLLM(messages, {
          model: 'gpt-4-turbo',
          temperature: 0.7,
          maxTokens: 1000,
        });

        return {
          content: response.content,
          usage: response.usage,
          cost: response.cost,
          model: 'gpt-4-turbo',
        };
      },
    },
    
    booking: {
      name: 'BookingAgent',
      description: 'Handles appointment booking and therapist search',
      systemPrompt: `You are BookingAgent, specialized in helping users book therapy appointments.
You can:
- Search for therapists by specialty, location, availability
- Show therapist profiles and ratings
- Check availability and book appointments
- Handle rescheduling and cancellations

Be conversational and guide users through the booking process step by step.`,
      execute: async ({ message, context, conversation, supabase }) => {
        // Placeholder - will be implemented in Wave 3
        return {
          content: 'BookingAgent is being set up. Please use the classic booking flow for now.',
          usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
          cost: 0,
        };
      },
    },
    
    // Other agents will be implemented in Wave 3
    session: {
      name: 'SessionAgent',
      description: 'Real-time copilot during therapy sessions',
      systemPrompt: 'SessionAgent system prompt...',
      execute: async () => ({ content: 'SessionAgent coming in Wave 3', usage: {}, cost: 0 }),
    },
    
    insights: {
      name: 'InsightsAgent',
      description: 'Dashboard intelligence and analytics',
      systemPrompt: 'InsightsAgent system prompt...',
      execute: async () => ({ content: 'InsightsAgent coming in Wave 3', usage: {}, cost: 0 }),
    },
    
    followup: {
      name: 'FollowupAgent',
      description: 'Post-session engagement and follow-ups',
      systemPrompt: 'FollowupAgent system prompt...',
      execute: async () => ({ content: 'FollowupAgent coming in Wave 3', usage: {}, cost: 0 }),
    },
  };
}
```

### Step 3: Integrate OpenAI & Anthropic APIs

**Related Ticket:** ticket:d969320e-d519-47a7-a258-e04789b8ce0e/691129ea-ffb8-4fb5-a003-42982e3ce350

**Already completed in Step 2.1** - The `llm-client.ts` file handles both OpenAI and Anthropic integration with automatic fallback.

**Testing:**

Create mobile/supabase/functions/test-llm/index.ts:

```typescript
// @ts-nocheck
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { callLLM } from '../_shared/llm-client.ts';
import { corsHeaders } from '../_shared/cors.ts';

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders });
  }

  try {
    const { message } = await req.json();

    const response = await callLLM(
      [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: message || 'Hello, how are you?' },
      ],
      {
        model: 'gpt-4-turbo',
        temperature: 0.7,
        maxTokens: 500,
      }
    );

    return new Response(
      JSON.stringify({
        response: response.content,
        usage: response.usage,
        cost: response.cost,
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  } catch (error) {
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});
```

### Step 4: Implement Feature Flags System

**Related Ticket:** ticket:d969320e-d519-47a7-a258-e04789b8ce0e/4142b62a-7963-4259-bc14-27961c08bfd7

#### 4.1 Create Feature Flag Hook (Web)

Create web/hooks/use-feature-flag.ts:

```typescript
'use client';

import { useState, useEffect } from 'react';
import { createBrowserClient } from '@/lib/supabase/client';

export function useFeatureFlag(flagName: string): boolean {
  const [isEnabled, setIsEnabled] = useState(false);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    async function checkFlag() {
      const supabase = createBrowserClient();
      
      // Get current user
      const { data: { user } } = await supabase.auth.getUser();
      if (!user) {
        setIsEnabled(false);
        setLoading(false);
        return;
      }

      // Check feature flag
      const { data, error } = await supabase.rpc('is_feature_enabled', {
        flag_name: flagName,
        check_user_id: user.id,
      });

      if (!error) {
        setIsEnabled(data || false);
      }
      
      setLoading(false);
    }

    checkFlag();
  }, [flagName]);

  return isEnabled;
}

// Usage example:
// const isAIChatEnabled = useFeatureFlag('ai_chat_enabled');
```

#### 4.2 Create Feature Flag Service (Mobile)

Create mobile/src/services/featureFlags.ts:

```typescript
import { supabase } from '../api/supabase';

class FeatureFlagService {
  private cache: Map<string, { value: boolean; timestamp: number }> = new Map();
  private cacheDuration = 5 * 60 * 1000; // 5 minutes

  async isEnabled(flagName: string, userId: string): Promise<boolean> {
    // Check cache
    const cached = this.cache.get(flagName);
    if (cached && Date.now() - cached.timestamp < this.cacheDuration) {
      return cached.value;
    }

    try {
      const { data, error } = await supabase.rpc('is_feature_enabled', {
        flag_name: flagName,
        check_user_id: userId,
      });

      if (error) {
        console.error('Feature flag error:', error);
        return false;
      }

      // Cache result
      this.cache.set(flagName, {
        value: data || false,
        timestamp: Date.now(),
      });

      return data || false;
    } catch (error) {
      console.error('Feature flag error:', error);
      return false;
    }
  }

  clearCache() {
    this.cache.clear();
  }
}

export const featureFlagService = new FeatureFlagService();
```

### Step 5: Deploy and Test

#### 5.1 Deploy Edge Functions

```bash
# Deploy orchestrator
cd mobile
supabase functions deploy agent-orchestrator

# Deploy test function
supabase functions deploy test-llm

# Test orchestrator
curl -X POST https://your-project.supabase.co/functions/v1/agent-orchestrator \
  -H "Authorization: Bearer YOUR_ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "I want to book an appointment for anxiety",
    "context": {
      "userId": "test-user-id",
      "screen": "/home"
    }
  }'
```

#### 5.2 Test Feature Flags

```bash
# Enable a feature flag
psql -h localhost -p 54322 -U postgres -d postgres -c "
  UPDATE feature_flags 
  SET enabled = true, rollout_percentage = 10 
  WHERE name = 'ai_alpha_users';
"

# Test flag evaluation
psql -h localhost -p 54322 -U postgres -d postgres -c "
  SELECT is_feature_enabled('ai_alpha_users', 'test-user-uuid');
"
```

## Success Criteria

### Database Schema

- ✅ All 8 new tables created successfully
- ✅ pgvector extension enabled
- ✅ RLS policies enforced (test cross-user access)
- ✅ Indexes created for performance
- ✅ Feature flag function works correctly

### LangGraph Orchestrator

- ✅ Orchestrator deployed and responding
- ✅ Intent classification accuracy > 80% (test with 10 sample messages)
- ✅ LLM fallback works (simulate OpenAI failure)
- ✅ Conversation history persisted correctly
- ✅ Execution logs captured with costs

### LLM Integration

- ✅ OpenAI API responding (< 2s latency)
- ✅ Anthropic API responding (< 2s latency)
- ✅ Fallback mechanism works
- ✅ Cost tracking accurate
- ✅ Token usage logged

### Feature Flags

- ✅ Flags togglable via database
- ✅ Percentage rollout works (10% = ~10% of users)
- ✅ User targeting works
- ✅ Client-side hooks work (web and mobile)

## Testing Checklist

```bash
# 1. Test database migrations
supabase db reset
supabase db push

# 2. Test orchestrator
curl -X POST https://your-project.supabase.co/functions/v1/agent-orchestrator \
  -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
  -d '{"message": "Hello", "context": {"userId": "test-id"}}'

# 3. Test intent classification
# Should classify as "booking"
curl -X POST .../agent-orchestrator \
  -d '{"message": "I want to book an appointment for anxiety"}'

# Should classify as "session"
curl -X POST .../agent-orchestrator \
  -d '{"message": "Can you help me with my SOAP notes?"}'

# 4. Test feature flags
# Enable flag
UPDATE feature_flags SET enabled = true WHERE name = 'ai_chat_enabled';

# Test in web app
# Navigate to app and check if AI chat appears

# 5. Test cost tracking
# Check agent_executions table for cost data
SELECT agent_type, AVG(cost_usd), SUM(tokens_used) 
FROM agent_executions 
GROUP BY agent_type;
```

## Dependencies & Execution Order

```mermaid
graph TD
    A[Step 1: Database Schema] --> B[Step 2: LangGraph Orchestrator]
    A --> C[Step 3: LLM Integration]
    A --> D[Step 4: Feature Flags]
    
    C --> B
    D --> B
    
    B --> E[Step 5: Deploy & Test]
    C --> E
    D --> E
```

**Execution Order:**

1. **Day 1-2:** Database schema (migrations 026-033)
2. **Day 2-3:** LLM client and orchestrator
3. **Day 3-4:** Intent classifier and agent registry
4. **Day 4-5:** Feature flags (database + client hooks)
5. **Day 5:** Deploy and comprehensive testing

## Related Tickets

**Core Dependencies:**

- ticket:d969320e-d519-47a7-a258-e04789b8ce0e/7d72e40c-fd7f-4083-99b0-ff10e93c4a25 - Database Schema
- ticket:d969320e-d519-47a7-a258-e04789b8ce0e/31fff4b4-78dc-43c0-ad41-f04770463328 - LangGraph Orchestrator
- ticket:d969320e-d519-47a7-a258-e04789b8ce0e/691129ea-ffb8-4fb5-a003-42982e3ce350 - LLM Integration
- ticket:d969320e-d519-47a7-a258-e04789b8ce0e/4142b62a-7963-4259-bc14-27961c08bfd7 - Feature Flags

**Blocks:**

- All Wave 2 tickets (require database schema)
- All Wave 3 tickets (require orchestrator)
- All frontend tickets (require feature flags)

## Monitoring & Rollbar Integration

Your existing Rollbar setup (mobile/supabase/functions/_shared/rollbar.ts) is already integrated. All agent operations will automatically report to Rollbar with trace IDs.

**View in Rollbar:**

- Navigate to Rollbar dashboard
- Filter by `context: orchestrator`
- View trace IDs for distributed tracing
- Monitor error rates and latency

## Cost Tracking

After Wave 1, you can monitor LLM costs:

```sql
-- Daily cost report
SELECT 
  DATE(created_at) as date,
  agent_type,
  COUNT(*) as executions,
  SUM(tokens_used) as total_tokens,
  SUM(cost_usd) as total_cost
FROM agent_executions
WHERE created_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE(created_at), agent_type
ORDER BY date DESC, total_cost DESC;
```

## Next Wave Preview

**Wave 2** will build on this foundation:

- Tool registry with validation
- PII masking for HIPAA compliance
- RAG system with pgvector
- Cost monitoring dashboard
- CI/CD pipeline


